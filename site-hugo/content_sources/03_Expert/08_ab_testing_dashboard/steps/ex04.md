```py
"""
Step 4: Add Visual Comparison Chart (Complete)
- Create bar chart comparing groups
- Visualize conversions vs non-conversions
- Complete the A/B testing dashboard
"""

import streamlit as st
import pandas as pd
import plotly.express as px
from scipy import stats

st.set_page_config(page_title="A/B Testing Dashboard", page_icon="üß™", layout="wide")

st.title("üß™ A/B Testing Dashboard")

st.sidebar.header("Test Configuration")

visitors_a = st.sidebar.number_input("Group A Visitors", 1000, 100000, 10000)
conversions_a = st.sidebar.number_input("Group A Conversions", 100, visitors_a, 1200)

visitors_b = st.sidebar.number_input("Group B Visitors", 1000, 100000, 10000)
conversions_b = st.sidebar.number_input("Group B Conversions", 100, visitors_b, 1500)

conv_rate_a = conversions_a / visitors_a
conv_rate_b = conversions_b / visitors_b

col1, col2, col3 = st.columns(3)

with col1:
    st.metric("Group A Conv. Rate", f"{conv_rate_a:.2%}")

with col2:
    st.metric("Group B Conv. Rate", f"{conv_rate_b:.2%}")

with col3:
    improvement = ((conv_rate_b - conv_rate_a) / conv_rate_a) * 100
    st.metric("Improvement", f"{improvement:+.2f}%")

# Statistical significance
z_score, p_value = stats.proportions_ztest(
    [conversions_a, conversions_b],
    [visitors_a, visitors_b]
)

st.subheader("üìä Statistical Significance")

col1, col2 = st.columns(2)

with col1:
    st.metric("P-Value", f"{p_value:.4f}")

with col2:
    is_significant = p_value < 0.05
    st.metric("Significant?", "Yes ‚úÖ" if is_significant else "No ‚ùå")

if is_significant:
    st.success("The difference is statistically significant!")
else:
    st.warning("The difference is not statistically significant.")

st.divider()

# STEP 1: Create visualization section
st.subheader("üìà Visual Comparison")

# STEP 2: Prepare data for visualization
# Create DataFrame with conversion data
df = pd.DataFrame({
    'Group': ['A', 'A', 'B', 'B'],
    'Type': ['Converted', 'Not Converted'] * 2,
    'Count': [
        conversions_a,
        visitors_a - conversions_a,
        conversions_b,
        visitors_b - conversions_b
    ]
})

# STEP 3: Create grouped bar chart with Plotly
fig = px.bar(
    df,
    x='Group',
    y='Count',
    color='Type',
    barmode='group',
    title="Conversion Comparison: Group A vs Group B",
    color_discrete_map={
        'Converted': '#2ecc71',      # Green for conversions
        'Not Converted': '#e74c3c'   # Red for non-conversions
    }
)

# STEP 4: Customize chart layout
fig.update_layout(
    xaxis_title="Test Group",
    yaxis_title="Number of Visitors",
    legend_title="Outcome",
    hovermode='x unified'
)

# Display chart
st.plotly_chart(fig, use_container_width=True)

# STEP 5: Add conversion rate comparison chart
st.subheader("üìä Conversion Rate Comparison")

rate_df = pd.DataFrame({
    'Group': ['Group A (Control)', 'Group B (Variant)'],
    'Conversion Rate': [conv_rate_a * 100, conv_rate_b * 100]
})

fig_rate = px.bar(
    rate_df,
    x='Group',
    y='Conversion Rate',
    title="Conversion Rate Comparison",
    color='Group',
    color_discrete_map={
        'Group A (Control)': '#3498db',
        'Group B (Variant)': '#9b59b6'
    }
)

fig_rate.update_layout(
    yaxis_title="Conversion Rate (%)",
    showlegend=False
)

# Add percentage labels on bars
fig_rate.update_traces(texttemplate='%{y:.2f}%', textposition='outside')

st.plotly_chart(fig_rate, use_container_width=True)

# STEP 6: Add summary insights
st.divider()

st.subheader("üí° Test Summary & Recommendations")

# Create insights based on results
col1, col2 = st.columns(2)

with col1:
    st.write("**Key Findings:**")
    st.write(f"- Group A: {conv_rate_a:.2%} conversion rate")
    st.write(f"- Group B: {conv_rate_b:.2%} conversion rate")
    st.write(f"- Difference: {improvement:+.2f}%")
    st.write(f"- P-value: {p_value:.4f}")
    st.write(f"- Sample size: {visitors_a + visitors_b:,} total visitors")

with col2:
    st.write("**Recommendation:**")

    if is_significant:
        if improvement > 0:
            st.success("""
            ‚úÖ **Implement Group B (Variant)**

            The variant shows significant improvement over the control.
            Rolling out this change is recommended.
            """)
        else:
            st.info("""
            ‚ÑπÔ∏è **Keep Group A (Control)**

            The control performs significantly better.
            Do not implement the variant.
            """)
    else:
        st.warning("""
        ‚ö†Ô∏è **Continue Testing**

        Results are not statistically significant. Consider:
        - Running test longer to collect more data
        - Increasing sample size
        - Testing a more impactful variant
        """)

# STEP 7: Add comprehensive guide
with st.expander("üìö A/B Testing Best Practices"):
    st.markdown("""
    ### Running Effective A/B Tests

    #### 1. Planning Phase
    - Define clear hypothesis
    - Set success metrics
    - Calculate required sample size
    - Determine test duration

    #### 2. During Test
    - Don't peek at results too early
    - Ensure proper randomization
    - Monitor for technical issues
    - Maintain consistent split

    #### 3. Analysis Phase
    - Check statistical significance
    - Consider practical significance
    - Look for segment differences
    - Validate with holdout group

    #### 4. Common Pitfalls

    **Peeking Problem:**
    - Checking results too early increases false positives
    - Wait for predetermined sample size

    **Sample Size:**
    - Too small = unreliable results
    - Calculate needed size beforehand

    **Multiple Testing:**
    - Running many tests increases false positives
    - Adjust alpha level (Bonferroni correction)

    **Seasonality:**
    - Account for day-of-week effects
    - Run tests for full weeks

    #### 5. Sample Size Calculator

    Required sample size depends on:
    - Baseline conversion rate
    - Minimum detectable effect
    - Significance level (alpha)
    - Statistical power (1 - beta)

    **Rule of thumb:**
    - Need ~400 conversions per variant
    - Higher for smaller effects

    #### 6. When to Stop Testing

    **Stop when:**
    - ‚úÖ Reached predetermined sample size
    - ‚úÖ Statistical significance achieved
    - ‚úÖ Practical significance confirmed

    **Don't stop just because:**
    - ‚ùå You like the current results
    - ‚ùå Results look "good enough"
    - ‚ùå Pressure to ship quickly
    """)

st.divider()
st.caption("Built with Streamlit üéà")

```